{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_regressor(df, columns, filed):\n",
    "    # Обучили регрессор.\n",
    "    X = df[df[filed].notnull()][columns]\n",
    "    y = df[df[filed].notnull()][filed]\n",
    "    model = LinearRegression()\n",
    "    model.fit(X, y)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_regressor(df, model, columns, filed):\n",
    "    # Получили значения для пустых значений.\n",
    "    X = df[df[filed].isnull()][columns]\n",
    "    y = model.predict(X)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emissions(data):\n",
    "    # Считаем диапазон за границами которого все выброс.    \n",
    "    q1 = data[:].quantile(0.25)\n",
    "    q3 = data[:].quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    return (q1 - 1.5 * iqr, q3 + 1.5 * iqr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_regressor_outlier(df, columns, filed):\n",
    "    # Обучили регрессор.\n",
    "    X = df[columns]\n",
    "    y = df[filed]\n",
    "    model = LinearRegression()\n",
    "    model.fit(X, y)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_regressor_outlier(df, model, columns, filed):\n",
    "    # Получили значения для пустых значений.\n",
    "    X = df[columns]\n",
    "    y = model.predict(X)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_outlier(data, field, columns):\n",
    "    # Обрабатываем выброс для конкретного поля.\n",
    "    r_1, r_2 = emissions(data[field])\n",
    "    if r_1 - r_2 != 0:\n",
    "        share = round(data[(r_1 > data[field]) | (data[field] > r_2)].size/data.size*100, 2)\n",
    "        print(f'{color.BOLD}\"{field}\"{color.END}\\nОбщая доля выбросов в процентах {share}%\\n')\n",
    "        if share > 5:\n",
    "            model = build_regressor_outlier(data[(r_1 < data[field]) & (data[field] < r_2)], columns, field)\n",
    "            data.loc[(r_1 > data[field]) | (data[field] > r_2), field] = predict_regressor_outlier(data[(r_1 > data[field]) | (data[field] > r_2)], model, columns, field)\n",
    "        else:\n",
    "            data.loc[(r_1 > data[field]) | (data[field] > r_2), field] = data[(r_1 < data[field]) & (data[field] < r_2)][field].mode()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_outlier_all(data, fields, columns):\n",
    "    # Обрабатываем выбросы для всех полей конкретной выборки\n",
    "    for field in fields:\n",
    "         clear_outlier(data, str(field), columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grouped_by_target(data, field, target):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.set(font_scale=1)\n",
    "    sns.countplot(x=field, hue=target, data=data)\n",
    "    plt.title(f'{color.BOLD}\"{field}\"{color.END} сгруппированная по целевой переменной')\n",
    "    plt.legend(title='Target', loc='upper right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grouped_by_target_kde(data, field, target, n=1000):\n",
    "    limit_bal_with_target_s = data[[field, target]]\n",
    "    limit_bal_s = limit_bal_with_target_s[field]\n",
    "    limit_bal_target_0 = limit_bal_s[limit_bal_with_target_s[target] == 0]\n",
    "    limit_bal_target_1 = limit_bal_s[limit_bal_with_target_s[target] == 1]\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    sns.kdeplot(limit_bal_target_0, shade=True, label='0', color='g')\n",
    "    sns.kdeplot(limit_bal_target_1, shade=True, label='1', color='r')\n",
    "    plt.xlabel(field)\n",
    "    plt.title(f'\"{field}\" сгруппированная по целевой переменной')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_into_category(df, field, target, var_1, var_2, id, significancy=0.05):\n",
    "    sample = df.loc[df[field].isin([var_1, var_2]), [id, field, target]]\n",
    "    table = sample.pivot_table(values=id, index=field, columns=target, aggfunc='count')\n",
    "    chi2, p, dof, expected = chi2_contingency(table, correction=False)\n",
    "    return p > significancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_into_category_table(df, field, target, id):\n",
    "    # Определить значения показателя, которые можно собрать в одно значение\n",
    "    values = df[field].unique()\n",
    "    values_len = len(values)\n",
    "    head =[' ']\n",
    "    head.extend(values)\n",
    "    result = [head]\n",
    "    for i in range(values_len):\n",
    "        row = [values[i]]\n",
    "        for j in range(values_len):\n",
    "            row.append(combine_into_category(df, field, target, values[i], values[j], id))\n",
    "        result.append(row)\n",
    "    dataToTable(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataToTable(data):\n",
    "    # Вывод таблицы\n",
    "    if len(data) == 0 :  \n",
    "        display(HTML('<b>Нет данных</b>'))\n",
    "        return\n",
    "    dt = ''\n",
    "    for i in range(len(data)):\n",
    "        row = data[i]\n",
    "        dt = dt + '<tr>'\n",
    "        for j in range(len(row)):\n",
    "            if j == 0:\n",
    "                dt = dt + '<td>' + str(row[j]) + '</td>'\n",
    "            elif i == j:\n",
    "                dt = dt + '<td bgcolor=\"#CCCCFF\"> </td>'\n",
    "            else:\n",
    "                if str(row[j]) == 'True':\n",
    "                    dt = dt + '<td bgcolor=\"#CCFFCC\" style=\"text-align: center; vertical-align: middle;\">' + str(row[j]) + '</td>'\n",
    "                elif str(row[j]) == 'False':\n",
    "                    dt = dt + '<td bgcolor=\"#FFCCCC\" style=\"text-align: center; vertical-align: middle;\">' + str(row[j]) + '</td>'\n",
    "                else:\n",
    "                    dt = dt + '<td>' + str(row[j]) + '</td>'\n",
    "        dt = dt + '</tr>'\n",
    "            \n",
    "    display(HTML('<table>' + dt + '</table>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bar_and_qq(data, column_name):\n",
    "    plt.figure(figsize=(20, 8))\n",
    "    ax1 = plt.subplot(121)\n",
    "    ax1.set_xlabel(column_name)\n",
    "    ax1.set_ylabel('Количество')\n",
    "    ax1.set_title(f'Распределение «{column_name}»')\n",
    "    data.hist()\n",
    "    plt.subplot(122)\n",
    "    probplot(data, dist='norm', plot=plt)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shapiro_wilk(data, column_name, significance=0.05):\n",
    "    t, pv = shapiro(data)\n",
    "    if pv < significance:\n",
    "        t, pv = shapiro(np.log(data))\n",
    "        if pv < significance:\n",
    "            print(f'Показатель «{column_name}» не соответствует нормальному распределнию.')\n",
    "        else:\n",
    "            print(f'Показатель «{column_name}» показатель нормализуется операцией логарифмирования.')\n",
    "    else:\n",
    "        print(f'Показатель «{column_name}» показатель нормально нормализован.')\n",
    "    print('Для достоверности произведём визуальную оценку распределения признака, а так же построим QQ-график.')\n",
    "    bar_and_qq(data, column_name)\n",
    "    return significance <= pv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confidence_interval_show(data, target_name, column_name, significance=0.05):\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.pointplot(x=target_name, y=column_name, data=data, capsize=.1)\n",
    "    plt.title(f'Доверительный интервал ({100 - significance} %) для «{column_name}»')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mann_whitneyu(data, target_name, column_name, significance=0.5):\n",
    "    data_with_target_s = data[[column_name, target_name]]\n",
    "    data_s = data_with_target_s[column_name]\n",
    "    data_target_0 = data_s[data_with_target_s[target_name] == 0]\n",
    "    data_target_1 = data_s[data_with_target_s[target_name] == 1]\n",
    "    t, pv = mannwhitneyu(data_target_0, data_target_1)\n",
    "    if (pv < significance):\n",
    "        print(f'По показателю «{column_name}» можно построить модель.')\n",
    "    else:\n",
    "        print(f'По показателю «{column_name}» нельзя построить модель.')\n",
    "    print('Для достоверности произведём визуальную оценку доверительного интервала.')\n",
    "    confidence_interval_show(data, target_name, column_name, significance)\n",
    "    return pv < significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_rate(data, target_name, column_name, show_kde=True):\n",
    "    display(Markdown(f'__Оценка признака «{column_name}» в разрезе целевой переменной__'))\n",
    "    if show_kde:\n",
    "        grouped_by_target_kde(data, column_name, target_name, df_train.shape[0])\n",
    "    shapiro_wilk(data[column_name], column_name)\n",
    "    return mann_whitneyu(data, target_name, column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_classification_report(y_train_true, y_train_pred, y_test_true, y_test_pred):\n",
    "    print('TRAIN\\n\\n' + classification_report(y_train_true, y_train_pred))\n",
    "    print('TEST\\n\\n' + classification_report(y_test_true, y_test_pred))\n",
    "    print('CONFUSION MATRIX\\n')\n",
    "    print(pd.crosstab(y_test_true, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_df_by_target(df, target_name):\n",
    "    target_counts = df[target_name].value_counts()\n",
    "    major_class_name = target_counts.argmax()\n",
    "    minor_class_name = target_counts.argmin()\n",
    "    disbalance_coeff = int(target_counts[major_class_name] / target_counts[minor_class_name]) - 1\n",
    "    for i in range(disbalance_coeff):\n",
    "        sample = df[df[target_name] == minor_class_name].sample(target_counts[minor_class_name])\n",
    "        df = df.append(sample, ignore_index=True)\n",
    "    return df.sample(frac=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_proba_calibration_plots(y_predicted_probs, y_true_labels):\n",
    "    preds_with_true_labels = np.array(list(zip(y_predicted_probs, y_true_labels)))\n",
    "    thresholds = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    f1_scores = []\n",
    "    for threshold in np.linspace(0.1, 0.9, 9):\n",
    "        thresholds.append(threshold)\n",
    "        precisions.append(precision_score(y_true_labels, list(map(int, y_predicted_probs > threshold))))\n",
    "        recalls.append(recall_score(y_true_labels, list(map(int, y_predicted_probs > threshold))))\n",
    "        f1_scores.append(f1_score(y_true_labels, list(map(int, y_predicted_probs > threshold))))\n",
    "    scores_table = pd.DataFrame({'f1':f1_scores,\n",
    "                                 'precision':precisions,\n",
    "                                 'recall':recalls,\n",
    "                                 'probability':thresholds}).sort_values('f1', ascending=False).round(3)\n",
    "    figure = plt.figure(figsize = (15, 5))\n",
    "    plt_calibration = figure.add_subplot(121)\n",
    "    plt_calibration.plot(thresholds, precisions, label='Precision', linewidth=4)\n",
    "    plt_calibration.plot(thresholds, recalls, label='Recall', linewidth=4)\n",
    "    plt_calibration.plot(thresholds, f1_scores, label='F1', linewidth=4)\n",
    "    plt_calibration.set_ylabel('Scores')\n",
    "    plt_calibration.set_xlabel('Probability threshold')\n",
    "    plt_calibration.set_title('Probabilities threshold calibration')\n",
    "    plt_calibration.legend(bbox_to_anchor=(0, 1))   \n",
    "    plt_calibration.table(cellText = scores_table.values,\n",
    "               colLabels = scores_table.columns, \n",
    "               colLoc = 'center', cellLoc = 'center', loc = 'bottom', bbox = [0, -1.3, 1, 1])\n",
    "    plt_histogram = figure.add_subplot(122)\n",
    "    plt_histogram.hist(preds_with_true_labels[preds_with_true_labels[:, 1] == 0][:, 0], \n",
    "              label='Another class', color='royalblue', alpha=1)\n",
    "    plt_histogram.hist(preds_with_true_labels[preds_with_true_labels[:, 1] == 1][:, 0], \n",
    "              label='Main class', color='darkcyan', alpha=0.8)\n",
    "    plt_histogram.set_ylabel('Number of examples')\n",
    "    plt_histogram.set_xlabel('Probabilities')\n",
    "    plt_histogram.set_title('Probability histogram')\n",
    "    plt_histogram.legend(bbox_to_anchor=(1, 1))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-51-3a2e4a3717da>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mdef\u001b[0m \u001b[0mshow_learning_curve_plot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_sizes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, \n\u001b[0;32m      3\u001b[0m                                                             \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                                                             \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'f1'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                                                             \u001b[0mtrain_sizes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_sizes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "def show_learning_curve_plot(estimator, X, y, cv=3, n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, \n",
    "                                                            cv=cv, \n",
    "                                                            scoring='f1',\n",
    "                                                            train_sizes=train_sizes, \n",
    "                                                            n_jobs=n_jobs)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.figure(figsize=(15,8))\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "    plt.title(f\"Learning curves ({type(estimator).__name__})\")\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")     \n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_feature_importances(feature_names, feature_importances, get_top=None):\n",
    "    feature_importances = pd.DataFrame({'feature': feature_names, 'importance': feature_importances})\n",
    "    feature_importances = feature_importances.sort_values('importance', ascending=False)\n",
    "    plt.figure(figsize = (20, len(feature_importances) * 0.355))\n",
    "    sns.barplot(feature_importances['importance'], feature_importances['feature'])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.title('Importance of features')\n",
    "    plt.show()\n",
    "    if get_top is not None:\n",
    "        return feature_importances['feature'][:get_top].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
